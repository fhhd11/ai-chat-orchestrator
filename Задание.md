# Task: Refactor and Extend FastAPI AI Chat Orchestrator

## Context & Goal

You need to refactor and significantly extend the existing FastAPI AI Chat Orchestrator to work with the new enhanced Supabase Edge Function API. The orchestrator serves as the main API gateway for an AI Chat Platform that provides access to multiple LLM models (GPT-4, Claude, Gemini) for users from CIS region.

## Current Architecture Overview

### System Components
- **FastAPI Orchestrator** (this project) - Main API gateway, streaming coordinator
- **Supabase Edge Functions** - Business logic, CRUD operations for conversations
- **LiteLLM Proxy** - Unified access to 100+ LLM providers with streaming support
- **Supabase Database** - PostgreSQL with RLS, user data, conversation history
- **Deployment** - Railway platform for both orchestrator and LiteLLM

### Current Repository Structure
```
ai-chat-orchestrator/
├── app/
│   ├── main.py              # FastAPI application
│   ├── config.py            # Configuration management
│   ├── models.py            # Pydantic models
│   ├── dependencies.py      # Dependency injection
│   ├── middleware.py        # Custom middleware
│   ├── routers/
│   │   ├── chat.py          # Chat completion endpoints
│   │   ├── conversations.py # Basic conversation management
│   │   └── health.py        # Health check endpoints
│   ├── services/
│   │   ├── supabase_client.py  # Supabase integration
│   │   ├── litellm_client.py   # LiteLLM streaming client
│   │   └── auth_service.py     # JWT authentication
│   └── utils/
│       ├── streaming.py     # SSE utilities
│       └── errors.py        # Custom exceptions
├── requirements.txt
├── railway.json
├── Dockerfile
└── README.md
```

## New Edge Function API Available

The Edge Function has been completely rewritten and now provides comprehensive REST API:

### Edge Function Base URL
`https://ptcpemfokwjgpjgmbgoj.supabase.co/functions/v1/conversation-manager`

### Available Endpoints

#### Legacy Endpoints (keep compatibility)
- `POST /init-conversation` - Create new conversation
- `POST /add-message` - Add user message
- `POST /save-response` - Save LLM response
- `POST /create-branch` - Create new branch
- `POST /switch-branch` - Switch active branch
- `GET|POST /build-context` - Build context for LLM

#### New REST Endpoints
**Conversations:**
- `GET /conversations?page=1&limit=20` - List user conversations with pagination
- `GET /conversations/{id}` - Get conversation basic info
- `GET /conversations/{id}/full` - Get conversation with all branches and messages
- `PATCH /conversations/{id}` - Update conversation (title, model)

**Branches:**
- `GET /conversations/{id}/branches` - List conversation branches
- `POST /conversations/{id}/branches` - Create branch from message
- `POST /conversations/{id}/branches/{branch_id}/activate` - Activate branch

**Messages:**
- `GET /messages/{id}` - Get message info
- `PATCH /messages/{id}` - Edit user message content
- `POST /messages/{id}/regenerate` - Regenerate assistant response

## LiteLLM Proxy Integration

### Base URL
`https://litellm-production-1c8b.up.railway.app`

### Available Endpoints
- `GET /models` - List all available models with metadata
- `POST /chat/completions` - Chat completions with streaming support
- Various provider-specific endpoints

## Task Requirements

### Phase 1: Architecture Refactoring

#### 1. Improve Project Structure
Enhance the current structure to better separate concerns:

```
app/
├── main.py                    # FastAPI app with improved middleware pipeline
├── config.py                  # Enhanced configuration with validation
├── dependencies.py            # Expanded dependency injection
├── middleware.py              # Enhanced middleware (CORS, auth, logging, metrics)
├── models/                    # Pydantic models split by domain
│   ├── __init__.py
│   ├── chat.py               # Chat completion models
│   ├── conversation.py       # Conversation, branch models
│   ├── message.py            # Message models  
│   ├── user.py               # User profile models
│   ├── common.py             # Pagination, responses, errors
│   └── litellm.py            # LiteLLM provider models
├── routers/                   # API routes split by domain
│   ├── __init__.py
│   ├── chat.py               # Streaming chat completions (existing, enhanced)
│   ├── conversations.py      # Full conversation management
│   ├── branches.py           # Branch operations
│   ├── messages.py           # Message operations
│   ├── models.py             # LLM models management
│   ├── users.py              # User profile and balance
│   └── health.py             # Health checks (existing, enhanced)
├── services/                  # Business logic layer
│   ├── __init__.py
│   ├── edge_proxy.py         # Universal Edge Function proxy
│   ├── litellm_service.py    # Enhanced LiteLLM integration
│   ├── auth_service.py       # JWT authentication (existing, enhanced)
│   ├── user_service.py       # User management business logic
│   ├── cache_service.py      # Redis caching layer
│   └── streaming_service.py  # SSE streaming utilities
├── utils/                     # Pure utilities
│   ├── __init__.py
│   ├── errors.py             # Custom exceptions (existing, enhanced)
│   ├── logging.py            # Structured logging utilities
│   ├── validators.py         # Custom Pydantic validators
│   └── metrics.py            # Prometheus metrics helpers
└── tests/                     # Comprehensive test suite
    ├── __init__.py
    ├── conftest.py           # Test configuration
    ├── test_chat.py          # Chat endpoints tests
    ├── test_conversations.py # Conversation management tests
    ├── test_auth.py          # Authentication tests
    └── test_integration.py   # End-to-end integration tests
```

#### 2. Enhanced Configuration Management
Update `config.py` to include all new settings:
- Edge Function endpoints configuration
- Redis configuration for caching
- Rate limiting settings
- Enhanced monitoring configuration
- Production vs development settings

### Phase 2: Core Services Implementation

#### 3. Universal Edge Function Proxy Service
Create `services/edge_proxy.py` with intelligent proxy functionality:
- **Dynamic endpoint mapping** - automatically proxy FastAPI routes to Edge Function routes
- **Request/Response transformation** - handle query parameters, pagination, etc.
- **Error handling** - translate Edge Function errors to appropriate HTTP status codes
- **JWT token forwarding** - seamlessly pass authentication tokens
- **Retry logic** - handle transient failures with exponential backoff
- **Request logging** - comprehensive request/response logging for debugging

#### 4. Enhanced LiteLLM Service
Extend `services/litellm_service.py`:
- **Models management** - fetch, cache, and format available models
- **Provider metadata** - include pricing, context windows, capabilities
- **Health monitoring** - track LiteLLM availability and performance
- **Streaming optimizations** - improve SSE performance and error handling

#### 5. Caching Service
Create `services/cache_service.py` for Redis integration:
- **Models caching** - cache LiteLLM models list (TTL: 1 hour)
- **User metadata caching** - cache user profiles and balance (TTL: 15 minutes)
- **Cache invalidation** - smart cache clearing on updates
- **Fallback strategy** - graceful degradation when Redis unavailable

#### 6. User Management Service
Create `services/user_service.py`:
- **Profile management** - get/update user profiles
- **Balance operations** - check balance, usage statistics
- **Spending analytics** - usage patterns, cost breakdowns

### Phase 3: API Routes Implementation

#### 7. Conversation Management Router
Create comprehensive `routers/conversations.py`:
- `GET /v1/conversations` - List with pagination, search, filtering
- `GET /v1/conversations/{id}` - Basic conversation info
- `GET /v1/conversations/{id}/full` - Complete conversation tree
- `PATCH /v1/conversations/{id}` - Update title, model, settings
- Proxy all requests to appropriate Edge Function endpoints

#### 8. Branch Management Router
Create `routers/branches.py`:
- `GET /v1/conversations/{id}/branches` - List branches with metadata
- `POST /v1/conversations/{id}/branches` - Create branch from message
- `POST /v1/conversations/{id}/branches/{branch_id}/activate` - Switch branch
- Include branch statistics (message count, creation date, etc.)

#### 9. Message Management Router
Create `routers/messages.py`:
- `GET /v1/messages/{id}` - Get message details
- `PATCH /v1/messages/{id}` - Edit user message
- `POST /v1/messages/{id}/regenerate` - Regenerate with new branch creation
- Support for message history and threading

#### 10. Models Management Router
Create `routers/models.py`:
- `GET /v1/models` - List all available models with rich metadata
- `GET /v1/models/{provider}` - Filter by provider (OpenAI, Anthropic, etc.)
- `GET /v1/models/{model_id}/info` - Detailed model information
- Include pricing, context limits, capabilities for each model

#### 11. Enhanced User Router
Expand `routers/users.py`:
- `GET /v1/user/profile` - User profile with preferences
- `GET /v1/user/balance` - Current balance and spending
- `GET /v1/user/usage` - Usage statistics and analytics
- `PATCH /v1/user/profile` - Update user preferences
- `GET /v1/user/conversations/stats` - Conversation statistics

### Phase 4: Advanced Features

#### 12. Enhanced Chat Router
Improve existing `routers/chat.py`:
- Better streaming performance with chunked responses
- Support for conversation context building through Edge Functions
- Enhanced error handling for streaming failures
- Request preprocessing and response postprocessing
- Integration with caching for model metadata

#### 13. Advanced Search and Filtering
- `GET /v1/conversations/search?q=query&model=gpt-4&date_from=2025-01-01`
- Full-text search through conversation titles and content
- Advanced filtering by model, date range, message count
- Aggregated search results with highlighting

#### 14. Batch Operations
- `POST /v1/conversations/batch` - Batch operations (update titles, change models)
- `DELETE /v1/conversations/batch` - Mass deletion with confirmation
- `POST /v1/export/conversations` - Export multiple conversations

### Phase 5: Production Enhancements

#### 15. Comprehensive Monitoring
- **Request metrics** by endpoint, user, model
- **Streaming metrics** - success rates, average duration, concurrent streams
- **Error tracking** - error rates by type and cause
- **Performance metrics** - response times, Edge Function latency
- **Business metrics** - token usage, cost tracking, user activity

#### 16. Security Hardening
- **Rate limiting** - per user, per endpoint limits using Redis
- **Request validation** - comprehensive input sanitization
- **Security headers** - CORS, CSP, security headers middleware
- **Audit logging** - security-relevant events tracking

#### 17. Developer Experience
- **Swagger documentation** with comprehensive examples and schemas
- **API client generation** - OpenAPI spec for client SDKs
- **Development tools** - debugging utilities, request replay
- **Integration guides** - documentation for frontend integration

## Technical Requirements

### Authentication & Security
- **JWT Token handling** - validate Supabase tokens, extract user context
- **Permission checking** - ensure users only access their own data
- **Secure proxy** - safely forward authentication to Edge Functions
- **Request validation** - validate all inputs with Pydantic models

### Performance Requirements
- **Streaming optimization** - maintain current excellent SSE performance
- **Caching strategy** - implement intelligent caching without stale data
- **Connection pooling** - efficient resource utilization
- **Async operations** - maintain async/await throughout the application

### Error Handling
- **Graceful degradation** - handle Edge Function or LiteLLM unavailability
- **Comprehensive logging** - structured logs with request tracing
- **User-friendly errors** - translate technical errors to meaningful messages
- **Circuit breaker** - prevent cascade failures

### Documentation
- **OpenAPI/Swagger** - comprehensive API documentation with examples
- **Response schemas** - detailed models for all endpoints
- **Error documentation** - document all possible error responses
- **Integration examples** - sample code for frontend integration

## API Design Principles

### 1. RESTful Design
- Use proper HTTP methods (GET, POST, PATCH, DELETE)
- Consistent URL structure and naming conventions
- Appropriate HTTP status codes for all scenarios

### 2. Consistent Response Format
```json
// Success responses
{
  "success": true,
  "data": { /* actual data */ }
}

// Paginated responses  
{
  "success": true,
  "data": {
    "items": [ /* array of items */ ],
    "pagination": {
      "page": 1,
      "limit": 20,
      "total": 45,
      "pages": 3
    }
  }
}

// Error responses
{
  "success": false,
  "error": {
    "message": "Human readable error message",
    "code": "ERROR_CODE",
    "details": { /* additional context */ }
  }
}
```

### 3. Comprehensive Validation
- Validate all input parameters with Pydantic
- Return clear validation errors with field-level details
- Sanitize inputs to prevent injection attacks

## Integration Points

### 1. Edge Function Integration
- **Base URL**: `https://ptcpemfokwjgpjgmbgoj.supabase.co/functions/v1/conversation-manager`
- **Authentication**: Forward JWT tokens in Authorization header
- **Error handling**: Map Edge Function responses to FastAPI responses
- **Request transformation**: Handle query parameters and body formatting

### 2. LiteLLM Integration  
- **Base URL**: `https://litellm-production-1c8b.up.railway.app`
- **Models endpoint**: `/models` for available models
- **Chat endpoint**: `/chat/completions` for streaming responses
- **Authentication**: Use user's litellm_key from database

### 3. Supabase Direct Integration
- **JWT validation** using Supabase auth
- **User profile access** for balance checking
- **Database connection** for critical operations

## Environment Configuration

Required environment variables:
```env
# Supabase
SUPABASE_URL=https://ptcpemfokwjgpjgmbgoj.supabase.co
SUPABASE_ANON_KEY=...
SUPABASE_SERVICE_KEY=...
EDGE_FUNCTION_URL=https://ptcpemfokwjgpjgmbgoj.supabase.co/functions/v1/conversation-manager

# LiteLLM  
LITELLM_URL=https://litellm-production-1c8b.up.railway.app
LITELLM_MASTER_KEY=...

# Redis (optional, for caching)
REDIS_URL=redis://localhost:6379

# Security
JWT_SECRET_KEY=...
JWT_ALGORITHM=HS256

# Performance
MAX_CONTEXT_MESSAGES=100
STREAM_TIMEOUT=120
CONNECTION_POOL_SIZE=100

# Monitoring
ENABLE_METRICS=true
LOG_LEVEL=INFO
```

## Key Functional Requirements

### 1. Maintain Existing Functionality
**CRITICAL**: All existing endpoints must continue working without breaking changes:
- `POST /v1/chat/completions` - Streaming chat with SSE
- `POST /v1/chat/regenerate` - Response regeneration
- `GET /v1/conversations/{id}` - Basic conversation info
- All health check endpoints

### 2. Add New REST API Endpoints

#### Conversation Management
- `GET /v1/conversations` - Paginated list with optional search/filtering
- `GET /v1/conversations/{id}/full` - Complete conversation with tree structure
- `PATCH /v1/conversations/{id}` - Update conversation properties

#### Branch & Message Management
- `GET /v1/conversations/{id}/branches` - List all branches
- `POST /v1/conversations/{id}/branches` - Create new branch
- `POST /v1/conversations/{id}/branches/{branch_id}/activate` - Switch branch
- `GET /v1/messages/{id}` - Message details
- `PATCH /v1/messages/{id}` - Edit user messages
- `POST /v1/messages/{id}/regenerate` - Regenerate responses

#### Models & User Management
- `GET /v1/models` - Available models with metadata (from LiteLLM)
- `GET /v1/user/profile` - User profile and preferences
- `GET /v1/user/balance` - Balance and spending information
- `GET /v1/user/usage` - Usage statistics and analytics

### 3. Universal Edge Function Proxy
Create intelligent proxy service that:
- **Automatically routes** FastAPI requests to corresponding Edge Function endpoints
- **Handles authentication** by forwarding JWT tokens
- **Transforms responses** to match FastAPI response format
- **Manages errors** with proper HTTP status code mapping
- **Logs requests** for debugging and monitoring

### 4. Enhanced Streaming Support
Maintain and improve the existing streaming capabilities:
- **SSE optimization** for better real-time performance  
- **Connection management** with proper cleanup
- **Error recovery** for interrupted streams
- **Concurrent streaming** support for multiple users

## Quality Requirements

### 1. Documentation Excellence
- **Comprehensive Swagger/OpenAPI** documentation with detailed descriptions
- **Request/Response examples** for every endpoint
- **Error response documentation** with all possible error codes
- **Integration guides** for frontend developers
- **API versioning strategy** for future compatibility

### 2. Production Readiness
- **Comprehensive error handling** with graceful degradation
- **Structured logging** with request tracing
- **Health checks** for all dependencies (Edge Functions, LiteLLM, Redis)
- **Metrics collection** for monitoring and alerting
- **Performance optimization** for high-load scenarios

### 3. Developer Experience
- **Type safety** with comprehensive Pydantic models
- **Async best practices** throughout the codebase
- **Clean architecture** with proper separation of concerns
- **Testable code** with dependency injection and mocking support
- **Clear code organization** with logical module structure

### 4. Security & Reliability
- **JWT token validation** on all protected endpoints
- **Request validation** with detailed error messages
- **Rate limiting ready** infrastructure (implement when needed)
- **Security headers** and CORS configuration
- **Input sanitization** to prevent injection attacks

## Implementation Strategy

### Step 1: Core Infrastructure
1. Refactor project structure with new directory organization
2. Create enhanced configuration management
3. Implement universal Edge Function proxy service
4. Update existing routers to use new proxy service

### Step 2: New API Endpoints  
1. Implement conversation management endpoints
2. Add branch and message management
3. Create models and user management endpoints
4. Ensure all endpoints proxy correctly to Edge Functions

### Step 3: Enhanced Features
1. Add comprehensive caching with Redis
2. Implement advanced search and filtering
3. Add batch operations support
4. Create detailed usage analytics

### Step 4: Production Polish
1. Enhance Swagger documentation with examples
2. Add comprehensive monitoring and metrics
3. Implement security hardening
4. Create integration tests and documentation

## Success Criteria

### Functional Success
- ✅ All existing endpoints work without breaking changes
- ✅ All new REST endpoints implemented and working
- ✅ Streaming performance maintained or improved
- ✅ Edge Function integration working seamlessly
- ✅ LiteLLM models integration functional

### Quality Success  
- ✅ Comprehensive Swagger documentation with examples
- ✅ All endpoints have proper Pydantic models
- ✅ Structured error handling with meaningful messages
- ✅ Request/Response logging for debugging
- ✅ Health checks for all dependencies

### Production Readiness
- ✅ Performance optimizations implemented
- ✅ Caching strategy working (if Redis available)
- ✅ Security headers and validation in place
- ✅ Monitoring and metrics collection active
- ✅ Integration tests passing

## Testing Requirements

### Unit Tests
- Test all service classes with mocked dependencies
- Test Pydantic model validation and serialization
- Test error handling scenarios

### Integration Tests
- Test Edge Function proxy with real endpoints
- Test LiteLLM integration with available models
- Test streaming functionality end-to-end
- Test authentication flow with valid/invalid tokens

### API Tests
- Test all REST endpoints with various input scenarios
- Test pagination and filtering functionality
- Test error responses and edge cases
- Test concurrent requests and streaming

## Deployment Considerations

### Railway Compatibility
- Maintain existing `railway.json` configuration
- Ensure Docker build process works with new structure
- Update environment variable requirements
- Test deployment process with new features

### Performance Optimization
- **Async/await** throughout the application
- **Connection pooling** for HTTP clients
- **Response compression** for large payloads
- **Efficient memory usage** for streaming operations

### Monitoring & Observability
- **Structured logging** with JSON format for production
- **Request tracing** with unique request IDs
- **Performance metrics** for all endpoints
- **Error tracking** with detailed context

## Expected Deliverables

1. **Refactored project structure** with clear separation of concerns
2. **Universal Edge Function proxy** service for seamless integration
3. **All new REST endpoints** implemented and documented
4. **Enhanced Pydantic models** for all data structures
5. **Comprehensive Swagger documentation** with examples
6. **Production-ready error handling** and logging
7. **Caching infrastructure** ready for Redis integration
8. **Complete test suite** for all functionality
9. **Updated README** with new API documentation
10. **Migration guide** for transitioning from old to new endpoints

The refactored orchestrator will serve as a complete, production-ready API gateway that provides elegant access to the AI Chat Platform's full functionality while maintaining excellent performance and developer experience.