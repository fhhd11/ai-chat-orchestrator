# Техническое задание: FastAPI Orchestrator для AI Chat Platform

## Контекст проекта

Необходимо создать микросервис-оркестратор на FastAPI, который связывает Supabase Edge Functions с LiteLLM Proxy для обеспечения streaming чатов с различными LLM моделями. Микросервис будет развернут на Railway.

## Структура проекта

```
ai-chat-orchestrator/
├── app/
│   ├── __init__.py
│   ├── main.py                 # Точка входа FastAPI
│   ├── config.py               # Конфигурация и env переменные
│   ├── models.py               # Pydantic модели
│   ├── dependencies.py         # Зависимости FastAPI
│   ├── middleware.py           # CORS, логирование, метрики
│   ├── routers/
│   │   ├── __init__.py
│   │   ├── chat.py            # Chat endpoints
│   │   ├── conversations.py   # Управление диалогами
│   │   └── health.py          # Health checks
│   ├── services/
│   │   ├── __init__.py
│   │   ├── supabase_client.py # Клиент для Edge Functions
│   │   ├── litellm_client.py  # Streaming от LiteLLM
│   │   └── auth_service.py    # Проверка токенов
│   └── utils/
│       ├── __init__.py
│       ├── streaming.py       # SSE утилиты
│       └── errors.py          # Обработка ошибок
├── tests/
│   ├── __init__.py
│   ├── test_chat.py
│   └── test_integration.py
├── requirements.txt
├── .env.example
├── railway.json
├── Dockerfile
└── README.md
```

## Технические требования

### 1. Основные зависимости

```python
# requirements.txt
fastapi==0.110.0
uvicorn[standard]==0.27.0
httpx==0.26.0
pydantic==2.5.0
pydantic-settings==2.1.0
python-jose[cryptography]==3.3.0
python-multipart==0.0.6
sse-starlette==2.0.0
prometheus-fastapi-instrumentator==6.1.0
python-dotenv==1.0.0
pytest==7.4.0
pytest-asyncio==0.21.0
```

### 2. Конфигурация (config.py)

```python
from pydantic_settings import BaseSettings
from typing import Optional

class Settings(BaseSettings):
    # API Settings
    app_name: str = "AI Chat Orchestrator"
    version: str = "1.0.0"
    debug: bool = False
    
    # Supabase
    supabase_url: str
    supabase_anon_key: str
    supabase_service_key: Optional[str] = None
    edge_function_url: str
    
    # LiteLLM
    litellm_url: str
    litellm_master_key: Optional[str] = None
    
    # Security
    jwt_secret_key: str
    jwt_algorithm: str = "HS256"
    
    # Performance
    max_context_messages: int = 100
    stream_timeout: int = 120
    connection_pool_size: int = 100
    
    # Monitoring
    enable_metrics: bool = True
    log_level: str = "INFO"
    
    class Config:
        env_file = ".env"
```

### 3. Модели данных (models.py)

```python
from pydantic import BaseModel, Field
from typing import Optional, List, Dict, Any
from enum import Enum

class Role(str, Enum):
    USER = "user"
    ASSISTANT = "assistant"
    SYSTEM = "system"

class ChatMessage(BaseModel):
    role: Role
    content: str

class ChatCompletionRequest(BaseModel):
    conversation_id: Optional[str] = None
    message: str
    model: Optional[str] = None
    temperature: Optional[float] = Field(0.7, ge=0, le=2)
    max_tokens: Optional[int] = Field(None, gt=0)
    stream: bool = True
    parent_message_id: Optional[str] = None

class RegenerateRequest(BaseModel):
    conversation_id: str
    message_id: str
    model: Optional[str] = None
    temperature: Optional[float] = None

class ConversationResponse(BaseModel):
    conversation_id: str
    branch_id: str
    message_id: str
    
class ErrorResponse(BaseModel):
    error: str
    details: Optional[Dict[str, Any]] = None
    code: Optional[str] = None
```

### 4. Основные эндпоинты

#### POST /v1/chat/completions
- **Описание**: Основной endpoint для streaming чата
- **Auth**: Bearer token (Supabase JWT)
- **Request Body**: ChatCompletionRequest
- **Response**: SSE stream или JSON
- **Функционал**:
  1. Валидация токена пользователя
  2. Получение litellm_key из user_profiles
  3. Вызов Edge Function add-message
  4. Вызов Edge Function build-context
  5. Streaming запрос к LiteLLM
  6. Проксирование SSE chunks клиенту
  7. Сохранение полного ответа через Edge Function save-response
  8. Обработка ошибок с graceful degradation

#### POST /v1/chat/regenerate
- **Описание**: Регенерация ответа
- **Auth**: Bearer token
- **Request Body**: RegenerateRequest
- **Response**: SSE stream или JSON
- **Функционал**:
  1. Создание новой ветки через Edge Function create-branch
  2. Новый запрос к LiteLLM
  3. Сохранение в новой ветке

#### GET /v1/conversations/{conversation_id}
- **Описание**: Получение информации о диалоге
- **Auth**: Bearer token
- **Response**: JSON с деревом сообщений

#### GET /health
- **Response**: {"status": "healthy", "version": "1.0.0", "services": {...}}

### 5. Клиент для Edge Functions (services/supabase_client.py)

```python
class SupabaseClient:
    def __init__(self, settings: Settings):
        self.base_url = settings.edge_function_url
        self.client = httpx.AsyncClient(
            timeout=30.0,
            limits=httpx.Limits(max_connections=100)
        )
    
    async def call_edge_function(
        self, 
        endpoint: str, 
        data: dict, 
        user_token: str
    ) -> dict:
        """
        Универсальный метод для вызова Edge Functions
        с retry логикой и обработкой ошибок
        """
        # Реализовать:
        # - 3 retry попытки с exponential backoff
        # - Логирование запросов/ответов
        # - Обработка различных HTTP статусов
        # - Timeout handling
```

### 6. LiteLLM Streaming Client (services/litellm_client.py)

```python
class LiteLLMClient:
    def __init__(self, settings: Settings):
        self.base_url = settings.litellm_url
        self.client = httpx.AsyncClient(
            timeout=httpx.Timeout(5.0, read=120.0),
            limits=httpx.Limits(max_connections=50)
        )
    
    async def stream_chat_completion(
        self,
        messages: List[dict],
        model: str,
        user_key: str,
        user_id: str,
        **kwargs
    ):
        """
        Streaming запрос к LiteLLM с обработкой SSE
        Должен yield SSE chunks и возвращать полный ответ
        """
        # Реализовать:
        # - SSE streaming с правильным форматированием
        # - Накопление полного ответа
        # - Обработка ошибок в потоке
        # - Heartbeat для поддержания соединения
        # - Подсчет токенов
```

### 7. Middleware и обработка ошибок

```python
# middleware.py
- CORS middleware с правильными настройками
- Request ID для трейсинга
- Логирование всех запросов
- Метрики (latency, request count, errors)
- Rate limiting (опционально)

# utils/errors.py
class ChatOrchestratorException(Exception):
    """Базовый класс для всех ошибок"""
    
class AuthenticationError(ChatOrchestratorException):
    """Ошибки аутентификации"""
    
class EdgeFunctionError(ChatOrchestratorException):
    """Ошибки при вызове Edge Functions"""
    
class LiteLLMError(ChatOrchestratorException):
    """Ошибки LiteLLM"""
    
class InsufficientBalanceError(ChatOrchestratorException):
    """Недостаточно средств"""
```

### 8. Основной chat endpoint (routers/chat.py)

```python
@router.post("/v1/chat/completions")
async def chat_completions(
    request: ChatCompletionRequest,
    authorization: str = Header(...),
    background_tasks: BackgroundTasks,
    supabase: SupabaseClient = Depends(get_supabase_client),
    litellm: LiteLLMClient = Depends(get_litellm_client)
):
    """
    Основная логика:
    
    1. Извлечь и проверить JWT токен
    2. Получить user_id из токена
    3. Получить litellm_key из БД через Edge Function или кеш
    4. Проверить баланс пользователя
    5. Добавить сообщение пользователя
    6. Построить контекст
    7. Начать streaming от LiteLLM
    8. В background task сохранить ответ
    
    При stream=false вернуть полный ответ в JSON
    """
```

### 9. Обработка SSE Streaming

```python
async def generate_sse_response(
    request_data: dict,
    user_token: str,
    litellm_key: str
):
    """
    Генератор для SSE streaming
    Формат chunks:
    data: {"choices": [{"delta": {"content": "..."}}]}\n\n
    
    В конце:
    data: [DONE]\n\n
    """
    try:
        # Streaming логика
        async for chunk in litellm_stream():
            yield f"data: {json.dumps(chunk)}\n\n"
    except Exception as e:
        yield f"data: {json.dumps({'error': str(e)})}\n\n"
    finally:
        yield "data: [DONE]\n\n"
```

### 10. Требования к обработке ошибок

1. **Graceful degradation**: При недоступности сервисов возвращать понятные ошибки
2. **Retry логика**: 3 попытки для Edge Functions, 2 для LiteLLM
3. **Circuit breaker**: Временное отключение сбойных сервисов
4. **Детальное логирование**: Все ошибки с контекстом
5. **User-friendly сообщения**: Не показывать технические детали клиенту

### 11. Кеширование

```python
# Использовать in-memory кеш для:
- JWT токенов (TTL: 5 минут)
- litellm_key пользователей (TTL: 10 минут)
- Информация о балансе (TTL: 1 минута)

from cachetools import TTLCache
cache = TTLCache(maxsize=1000, ttl=300)
```

### 12. Метрики и мониторинг

```python
# Prometheus метрики:
- request_duration_seconds
- request_count_total
- error_count_total
- active_streams_gauge
- litellm_tokens_total
- edge_function_calls_total
```

### 13. Конфигурация для Railway

```json
// railway.json
{
  "build": {
    "builder": "NIXPACKS"
  },
  "deploy": {
    "startCommand": "uvicorn app.main:app --host 0.0.0.0 --port $PORT",
    "restartPolicyType": "ON_FAILURE",
    "restartPolicyMaxRetries": 3
  }
}
```

### 14. Environment переменные

```env
# .env.example
# Supabase
SUPABASE_URL=https://ptcpemfokwjgpjgmbgoj.supabase.co
SUPABASE_ANON_KEY=
SUPABASE_SERVICE_KEY=
EDGE_FUNCTION_URL=https://ptcpemfokwjgpjgmbgoj.supabase.co/functions/v1/conversation-manager

# LiteLLM
LITELLM_URL=https://litellm-production-1c8b.up.railway.app
LITELLM_MASTER_KEY=

# Security
JWT_SECRET_KEY=

# Performance
MAX_CONTEXT_MESSAGES=100
STREAM_TIMEOUT=120
CONNECTION_POOL_SIZE=100
```

### 15. Тестовые сценарии

1. **Успешный streaming чат**
2. **Регенерация ответа**
3. **Создание нового диалога**
4. **Обработка недоступности LiteLLM**
5. **Обработка недоступности Edge Functions**
6. **Проверка истечения токена**
7. **Проверка недостаточного баланса**
8. **Concurrent запросы от одного пользователя**

### 16. Примеры curl запросов для тестирования

```bash
# Streaming chat
curl -X POST http://localhost:8000/v1/chat/completions \
  -H "Authorization: Bearer $TOKEN" \
  -H "Content-Type: application/json" \
  -d '{
    "message": "Hello, how are you?",
    "stream": true
  }'

# Non-streaming chat
curl -X POST http://localhost:8000/v1/chat/completions \
  -H "Authorization: Bearer $TOKEN" \
  -H "Content-Type: application/json" \
  -d '{
    "conversation_id": "uuid",
    "message": "Tell me more",
    "stream": false,
    "model": "gpt-4"
  }'
```

## Критерии готовности

1. ✅ Все эндпоинты работают корректно
2. ✅ SSE streaming без обрывов
3. ✅ Graceful error handling
4. ✅ Метрики доступны на /metrics
5. ✅ Тесты покрывают основные сценарии
6. ✅ README с инструкцией по запуску
7. ✅ Успешный деплой на Railway

## Дополнительные требования

- Код должен следовать PEP 8
- Использовать type hints везде
- Docstrings для всех функций
- Асинхронность везде где возможно
- Логирование с использованием structlog или loguru
- Не хардкодить конфиденциальные данные

---

**Это задание предоставляет полную спецификацию для реализации микросервиса. Начни с создания базовой структуры проекта, затем реализуй сервисы, роутеры и finally интеграцию с streaming.**





## Дополнение к ТЗ: Детальная спецификация интеграций

### A. Edge Functions API - Точная спецификация

#### База URL
```
https://ptcpemfokwjgpjgmbgoj.supabase.co/functions/v1/conversation-manager
```

#### Доступные endpoints и их использование:

##### 1. Добавление сообщения пользователя
```python
# POST /conversation-manager/add-message
# Headers: Authorization: Bearer {user_jwt_token}

# Request:
{
    "conversation_id": "uuid",  # Опционально, если нет - создаст новый
    "content": "Текст сообщения",
    "role": "user",  # всегда "user" для сообщений пользователя
    "parent_id": "uuid"  # Опционально, для ветвления
}

# Response:
{
    "success": true,
    "data": {
        "message_id": "0694bc2c-8cc9-4d81-af3d-c4e052813558",
        "conversation_id": "d3aa0f45-3dc7-4c26-bfef-0055ca602f98",
        "branch_id": "4e79cd5d-dec8-48ee-8346-7402fc978536",
        "parent_id": null,  # или UUID
        "is_new_conversation": false
    }
}
```

##### 2. Построение контекста
```python
# POST /conversation-manager/build-context
# Headers: Authorization: Bearer {user_jwt_token}

# Request:
{
    "conversation_id": "d3aa0f45-3dc7-4c26-bfef-0055ca602f98",
    "branch_id": "uuid",  # Опционально, по умолчанию active_branch_id
    "max_messages": 50  # Опционально, default: 50
}

# Response:
{
    "success": true,
    "data": {
        "messages": [
            {"role": "user", "content": "Hello"},
            {"role": "assistant", "content": "Hi there!"}
        ],
        "token_count": 150,
        "branch_id": "4e79cd5d-dec8-48ee-8346-7402fc978536",
        "model": "gpt-4"  # модель из настроек диалога
    }
}
```

##### 3. Сохранение ответа ассистента
```python
# POST /conversation-manager/save-response
# Headers: Authorization: Bearer {user_jwt_token}

# Request:
{
    "conversation_id": "d3aa0f45-3dc7-4c26-bfef-0055ca602f98",
    "branch_id": "4e79cd5d-dec8-48ee-8346-7402fc978536",
    "parent_id": "0694bc2c-8cc9-4d81-af3d-c4e052813558",  # message_id пользователя
    "content": "Полный текст ответа от LLM",
    "model": "gpt-4",
    "tokens_count": 250  # Опционально, если не передать - посчитает автоматически
}

# Response:
{
    "success": true,
    "data": {
        "message_id": "bd78dab0-58e7-48e7-9c26-58411f45379e",
        "tokens_count": 250
    }
}
```

##### 4. Создание ветки для регенерации
```python
# POST /conversation-manager/create-branch
# Headers: Authorization: Bearer {user_jwt_token}

# Request:
{
    "conversation_id": "d3aa0f45-3dc7-4c26-bfef-0055ca602f98",
    "from_message_id": "0694bc2c-8cc9-4d81-af3d-c4e052813558",  # от какого сообщения ветвление
    "name": "Alternative Response 1"  # Опционально
}

# Response:
{
    "success": true,
    "data": {
        "branch_id": "a130fb43-1d5e-48e3-a1e0-6757a21ed132",
        "branch_name": "Alternative Response 1",
        "parent_message_id": null  # или UUID
    }
}
```

### B. Получение litellm_key пользователя

```python
# Через Supabase REST API
# GET https://ptcpemfokwjgpjgmbgoj.supabase.co/rest/v1/user_profiles
# Headers: 
#   - Authorization: Bearer {user_jwt_token}
#   - apikey: {SUPABASE_ANON_KEY}

# Response:
[{
    "id": "user-uuid",
    "litellm_key": "sk-7yVI3Tp9LLjxjEkGdrTqvA",
    "email": "user@example.com",
    "spend": 0.5,
    "max_budget": 10.0,
    "available_balance": 9.5
}]
```

### C. LiteLLM API - Точная спецификация

#### База URL
```
https://litellm-production-1c8b.up.railway.app
```

#### Streaming Chat Completion
```python
# POST /chat/completions
# Headers: 
#   - Authorization: Bearer {litellm_key}  # НЕ jwt токен, а litellm_key!
#   - Content-Type: application/json

# Request:
{
    "model": "gpt-4",  # или любая другая поддерживаемая модель
    "messages": [
        {"role": "system", "content": "You are a helpful assistant"},
        {"role": "user", "content": "Hello"},
        {"role": "assistant", "content": "Hi there!"},
        {"role": "user", "content": "How are you?"}
    ],
    "stream": true,
    "temperature": 0.7,
    "max_tokens": 2000,
    "user": "d4d2bbd5-357c-4966-a689-b30cd2edbe79"  # user_id для трекинга
}

# Response (SSE stream):
data: {"id":"chatcmpl-123","object":"chat.completion.chunk","created":1677652288,"model":"gpt-4","choices":[{"index":0,"delta":{"role":"assistant","content":"I'm"},"finish_reason":null}]}

data: {"id":"chatcmpl-123","object":"chat.completion.chunk","created":1677652288,"model":"gpt-4","choices":[{"index":0,"delta":{"content":" doing"},"finish_reason":null}]}

data: {"id":"chatcmpl-123","object":"chat.completion.chunk","created":1677652288,"model":"gpt-4","choices":[{"index":0,"delta":{"content":" well"},"finish_reason":null}]}

data: {"id":"chatcmpl-123","object":"chat.completion.chunk","created":1677652288,"model":"gpt-4","choices":[{"index":0,"delta":{},"finish_reason":"stop"}],"usage":{"prompt_tokens":50,"completion_tokens":10,"total_tokens":60}}

data: [DONE]
```

### D. Полный flow обработки запроса

```python
async def process_chat_request(
    user_message: str,
    conversation_id: Optional[str],
    user_jwt_token: str,
    model: Optional[str] = None
):
    """
    Полный алгоритм обработки:
    """
    
    # 1. Извлечь user_id из JWT токена
    user_id = decode_jwt(user_jwt_token)["sub"]
    
    # 2. Получить litellm_key пользователя из БД
    async with httpx.AsyncClient() as client:
        response = await client.get(
            f"{SUPABASE_URL}/rest/v1/user_profiles?id=eq.{user_id}",
            headers={
                "Authorization": f"Bearer {user_jwt_token}",
                "apikey": SUPABASE_ANON_KEY
            }
        )
        user_profile = response.json()[0]
        litellm_key = user_profile["litellm_key"]
        available_balance = user_profile["available_balance"]
    
    # 3. Проверить баланс
    if available_balance is not None and available_balance <= 0:
        raise InsufficientBalanceError("Недостаточно средств")
    
    # 4. Добавить сообщение пользователя
    message_response = await call_edge_function(
        endpoint="add-message",
        data={
            "conversation_id": conversation_id,
            "content": user_message,
            "role": "user"
        },
        token=user_jwt_token
    )
    
    # 5. Получить контекст
    context_response = await call_edge_function(
        endpoint="build-context",
        data={
            "conversation_id": message_response["data"]["conversation_id"],
            "max_messages": 50
        },
        token=user_jwt_token
    )
    
    # 6. Подготовить запрос к LiteLLM
    litellm_request = {
        "model": model or context_response["data"]["model"],
        "messages": context_response["data"]["messages"],
        "stream": True,
        "user": user_id,
        "temperature": 0.7,
        "max_tokens": 2000
    }
    
    # 7. Streaming от LiteLLM
    full_response = ""
    async with httpx.AsyncClient() as client:
        async with client.stream(
            'POST',
            f"{LITELLM_URL}/chat/completions",
            json=litellm_request,
            headers={"Authorization": f"Bearer {litellm_key}"},
            timeout=120.0
        ) as response:
            async for line in response.aiter_lines():
                if line.startswith("data: "):
                    # Отправить chunk клиенту
                    yield line + "\n\n"
                    
                    # Парсить для накопления ответа
                    if line == "data: [DONE]":
                        break
                    
                    try:
                        chunk = json.loads(line[6:])
                        if chunk.get("choices") and chunk["choices"][0].get("delta"):
                            content = chunk["choices"][0]["delta"].get("content", "")
                            full_response += content
                    except json.JSONDecodeError:
                        continue
    
    # 8. Сохранить ответ в БД (в background task)
    await call_edge_function(
        endpoint="save-response",
        data={
            "conversation_id": message_response["data"]["conversation_id"],
            "branch_id": message_response["data"]["branch_id"],
            "parent_id": message_response["data"]["message_id"],
            "content": full_response,
            "model": model or context_response["data"]["model"]
        },
        token=user_jwt_token
    )
```

### E. Обработка ошибок от сервисов

#### Edge Functions ошибки:
```python
# 401 - Неверный токен
{"success": false, "error": "Authentication failed: ..."}

# 404 - Диалог не найден
{"success": false, "error": "Conversation not found or access denied"}

# 500 - Внутренняя ошибка
{"success": false, "error": "Error message"}
```

#### LiteLLM ошибки:
```python
# 429 - Rate limit
{"error": {"message": "Rate limit exceeded", "type": "rate_limit_error"}}

# 401 - Неверный ключ
{"error": {"message": "Invalid API key", "type": "authentication_error"}}

# 400 - Неверный запрос
{"error": {"message": "Invalid request", "type": "invalid_request_error"}}
```

### F. JWT токен Supabase

```python
# Структура декодированного токена:
{
    "aud": "authenticated",
    "exp": 1756442458,
    "sub": "d4d2bbd5-357c-4966-a689-b30cd2edbe79",  # user_id
    "email": "user@example.com",
    "role": "authenticated"
}

# Проверка токена:
from jose import jwt
decoded = jwt.decode(
    token, 
    SUPABASE_JWT_SECRET,  # Из Supabase Dashboard > Settings > API
    algorithms=["HS256"],
    audience="authenticated"
)
user_id = decoded["sub"]
```